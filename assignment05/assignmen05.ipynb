{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignmen05.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMnl9QSC9XC17sGiRNJnhb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trachtok/dspracticum2020_data/blob/main/assignment05/assignmen05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUNrMF9opFU7"
      },
      "source": [
        "# Assignment 05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Thlq97pLQE"
      },
      "source": [
        "## Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzpizA9wpFiz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from google.colab import drive\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHqfhUWgmZR6",
        "outputId": "da98343d-9336-426b-e93d-8e4c5fe9bd0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install split-folders\n",
        "import splitfolders"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.6/dist-packages (0.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEStg9RQpR67"
      },
      "source": [
        "## Prepare input data\n",
        "\n",
        "+ Mount Google Discs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSvyojdr-xAl",
        "outputId": "a14ff2fa-cfcf-4036-a74e-1b9def090393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7duj1bBpgzo"
      },
      "source": [
        "+ check what data we have, how many images in each class?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHvTSf-FoMzz",
        "outputId": "e0d1229c-1c5d-43af-eb82-c3f636a83607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/assignment05_data/batman' | wc -l\n",
        "!ls '/content/drive/My Drive/assignment05_data/superman' | wc -l\n",
        "!ls '/content/drive/My Drive/assignment05_data/spiderman' | wc -l"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n",
            "289\n",
            "284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-KMmofupzO3"
      },
      "source": [
        "+ to effective usage of `ImageDataGenerator()` it would be better to have 2 folders: training and validation a in both specific subset of images for classes batman, superman & spiderman\n",
        "+ to obtain such order, we can use package `splitfolders`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kteJZnhnqHqN",
        "outputId": "e2cbfa7d-c16d-41cc-b152-9fe5b78fc837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "splitfolders.ratio(\"/content/drive/My Drive/assignment05_data\", output=\"/content/drive/My Drive/assignment05_data/prepared_data\", seed=1337, ratio=(.8, .2)) "
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Copying files: 0 files [00:00, ? files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 6 files [00:00, 58.41 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 19 files [00:00, 60.11 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 31 files [00:00, 70.12 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 37 files [00:00, 52.61 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 47 files [00:00, 61.22 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 54 files [00:00, 49.43 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 64 files [00:01, 57.80 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 73 files [00:01, 49.85 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 86 files [00:01, 61.02 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 94 files [00:01, 53.69 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 108 files [00:01, 59.32 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 121 files [00:01, 70.72 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 130 files [00:01, 69.16 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 143 files [00:02, 80.29 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 153 files [00:02, 66.31 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 162 files [00:02, 64.84 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 173 files [00:02, 73.86 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 182 files [00:02, 60.26 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 195 files [00:02, 71.72 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 204 files [00:02, 70.81 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 213 files [00:03, 69.66 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 227 files [00:03, 81.27 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 237 files [00:03, 42.98 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 247 files [00:03, 43.93 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 259 files [00:03, 54.13 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 268 files [00:04, 58.67 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 279 files [00:04, 67.97 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 288 files [00:04, 63.49 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 296 files [00:04, 65.18 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 304 files [00:04, 53.37 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 317 files [00:04, 58.83 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 328 files [00:04, 68.32 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 337 files [00:05, 69.75 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 348 files [00:05, 78.32 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 357 files [00:05, 75.68 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 369 files [00:05, 76.61 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 382 files [00:05, 86.58 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 392 files [00:05, 79.65 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 403 files [00:05, 77.84 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 413 files [00:05, 82.70 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 422 files [00:06, 39.52 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 430 files [00:06, 46.52 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 438 files [00:06, 43.26 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 451 files [00:06, 49.51 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 463 files [00:07, 59.76 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 471 files [00:07, 49.33 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 483 files [00:07, 54.46 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 494 files [00:07, 64.16 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 503 files [00:07, 66.96 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 512 files [00:07, 69.39 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 520 files [00:08, 58.96 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 529 files [00:08, 65.66 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 537 files [00:08, 54.35 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 546 files [00:08, 56.19 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 553 files [00:08, 34.32 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 561 files [00:08, 41.18 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 567 files [00:09, 33.96 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 576 files [00:09, 41.02 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 582 files [00:09, 25.05 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 592 files [00:09, 31.90 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 600 files [00:10, 33.01 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 608 files [00:10, 39.79 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 617 files [00:10, 39.57 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 628 files [00:10, 48.89 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 635 files [00:10, 44.28 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 646 files [00:10, 53.89 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 654 files [00:11, 47.02 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 666 files [00:11, 56.64 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 674 files [00:11, 49.61 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 685 files [00:11, 49.34 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 699 files [00:11, 60.89 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 708 files [00:11, 57.85 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 719 files [00:12, 55.03 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 728 files [00:12, 61.55 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 736 files [00:12, 64.37 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 744 files [00:12, 36.65 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 754 files [00:12, 40.87 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 760 files [00:13, 45.15 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 770 files [00:13, 45.67 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 779 files [00:13, 52.76 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 787 files [00:13, 47.45 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 800 files [00:13, 58.27 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 808 files [00:13, 58.08 files/s]\u001b[A\u001b[A\n",
            "\n",
            "Copying files: 829 files [00:14, 58.99 files/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_btiT1Sqt4P",
        "outputId": "3ffb72af-dedf-4f63-c96b-9d745a9c6182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive/assignment05_data/prepared_data\"\n",
        "!ls \"/content/drive/My Drive/assignment05_data/prepared_data/train\"\n",
        "!ls \"/content/drive/My Drive/assignment05_data/prepared_data/val\""
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train  val\n",
            "batman\tsuperman\n",
            "batman\tsuperman\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvtHx2YtszUp"
      },
      "source": [
        "train_dir = \"/content/drive/My Drive/assignment05_data/prepared_data/train\"\n",
        "validation_dir = \"/content/drive/My Drive/assignment05_data/prepared_data/val\""
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssnB-innGKsk"
      },
      "source": [
        "# MobileNetV2\n",
        "+ download MobileNetV2 model without the top layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYiOFj_Lq6k8"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "IMG_SIZE = (160, 160)\n",
        "IMG_SHAPE = IMG_SIZE + (3,)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da-ZD6LQGOe7"
      },
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "# base_model.summary()"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cz_0D2jGb-S"
      },
      "source": [
        "# Transfer learning\n",
        "+ using already trained model, first freeze all layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2RP5aXVGbXF"
      },
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_PbCOJirdDy"
      },
      "source": [
        "+ then add pooling layer and dropout, finallu put on top `softmax` for categorical classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOK4su6DraJb"
      },
      "source": [
        "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = tf.keras.layers.Dense(units=2, activation=\"softmax\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGFRhLqKrwWN"
      },
      "source": [
        "+ create `ImageDataGenerator()` instance separately for training a validation set\n",
        "+ for training dataset, we can set several data augmentation parameters such as rotation, horizontal flip etc. -> it is not generally advised to do the same also for validation dataset, therefore, for validation only neccessary rescaling is set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sm2P0VJGiGG"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4CPAHwpsgfa"
      },
      "source": [
        "+ create iterator through training and validation dataset with `.flow_from_directory()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jTJ1UkLGkb4",
        "outputId": "50c88a7f-0bd0-4b5a-92df-f01f6309d73c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle = True)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', \n",
        "    shuffle=True)\n"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 435 images belonging to 2 classes.\n",
            "Found 110 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV-7e9fDIUEF",
        "outputId": "15041bdf-41e0-4004-a7ce-9283453fb7c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(train_generator,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=validation_generator)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 8/14 [================>.............] - ETA: 6s - loss: 0.9307 - accuracy: 0.5273"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 21s 2s/step - loss: 0.8196 - accuracy: 0.5885 - val_loss: 0.5301 - val_accuracy: 0.7091\n",
            "Epoch 2/5\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.4677 - accuracy: 0.7885 - val_loss: 0.4222 - val_accuracy: 0.7818\n",
            "Epoch 3/5\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.3561 - accuracy: 0.8391 - val_loss: 0.3563 - val_accuracy: 0.8545\n",
            "Epoch 4/5\n",
            "14/14 [==============================] - 18s 1s/step - loss: 0.2753 - accuracy: 0.8736 - val_loss: 0.3375 - val_accuracy: 0.8727\n",
            "Epoch 5/5\n",
            "14/14 [==============================] - 19s 1s/step - loss: 0.2486 - accuracy: 0.8897 - val_loss: 0.3279 - val_accuracy: 0.8727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJh_0dAVu3y-"
      },
      "source": [
        "## Export to TSjs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pim4RCMpu8eJ",
        "outputId": "7f3a0c33-5fd5-4adc-bf34-5bf22e45d920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflowjs\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/c8/c52e21c49b3baf0845e395241046a993e244dd4b94c9827a8cd2d9b18927/tensorflowjs-2.7.0-py3-none-any.whl (62kB)\n",
            "\r\u001b[K     |█████▎                          | 10kB 25.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 20kB 15.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 30kB 11.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 51kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 61kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (1.15.0)\n",
            "Collecting tensorflow-hub<0.10,>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/83/a7df82744a794107641dad1decaad017d82e25f0e1f761ac9204829eef96/tensorflow_hub-0.9.0-py2.py3-none-any.whl (103kB)\n",
            "\r\u001b[K     |███▏                            | 10kB 27.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20kB 33.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 37.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 40kB 21.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51kB 20.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 18.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71kB 17.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 81kB 16.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 92kB 15.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 102kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 16.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.10.0)\n",
            "Requirement already satisfied: tensorflow<3,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub<0.10,>=0.7.0->tensorflowjs) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub<0.10,>=0.7.0->tensorflowjs) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.1.2)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (1.33.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<3,>=2.1.0->tensorflowjs) (0.35.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-hub<0.10,>=0.7.0->tensorflowjs) (50.3.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.7.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2020.6.20)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (2.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<3,>=2.1.0->tensorflowjs) (3.1.0)\n",
            "Installing collected packages: tensorflow-hub, tensorflowjs\n",
            "  Found existing installation: tensorflow-hub 0.10.0\n",
            "    Uninstalling tensorflow-hub-0.10.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.10.0\n",
            "Successfully installed tensorflow-hub-0.9.0 tensorflowjs-2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfhtvH0ku9SS",
        "outputId": "f722d456-a230-4b84-b70b-e1a21aae66db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflowjs as tfjs\n",
        "tfjs.converters.save_keras_model(model, \"/content/drive/My Drive/assignment05_data/export_model\")"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/keras_h5_conversion.py:123: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  return h5py.File(h5file)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWKk6c3fvP-v",
        "outputId": "4da42e79-ab34-402b-feef-e2fcf6e786ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"export_model\", 'zip', \"/content/drive/My Drive/assignment05_data/export_model/\", )\n",
        "\n",
        "from google.colab import files\n",
        "files.download('export_model.zip')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9eb2477d-b0b8-4632-9d49-44c81abcc171\", \"export_model.zip\", 8418057)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}