{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment08.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHJOGoapQ0tx"
      },
      "source": [
        "# Classification of genomic sequences\n",
        "\n",
        "+ heavily inspired by https://www.tensorflow.org/tutorials/keras/text_classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixypWD6hQzul"
      },
      "source": [
        "!pip install biopython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkjeri08BtLg"
      },
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "from Bio import SeqIO\n",
        "import numpy as np\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oIYHKV5B0ZQ"
      },
      "source": [
        "## Reshaping data from FASTA to txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtyogPmKBu-w"
      },
      "source": [
        "classes = ['notpresent', 'present']\n",
        "sets = ['train', 'valid']\n",
        "\n",
        "for c in classes:\n",
        "    for s in sets:\n",
        "        urllib.request.urlretrieve(f\"https://github.com/simecek/dspracticum2020/raw/master/lecture_08/assignment/g4/g4_{c}_{s}.fa.gz\", f\"g4_{c}_{s}.fa.gz\")\n",
        "\n",
        "for c in classes:\n",
        "    for s in sets:\n",
        "        Path(f\"data/{s}/{c}\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for c in classes:\n",
        "    for s in sets:\n",
        "        with gzip.open(f\"g4_{c}_{s}.fa.gz\", \"rt\") as handle:\n",
        "            for record in SeqIO.parse(handle, \"fasta\"):\n",
        "                id = record.id\n",
        "                with open(f\"data/{s}/{c}/{id}.txt\", \"w\") as fw:\n",
        "                    fw.writelines([\" \".join(str(record.seq))])\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCdQ9v_TCJTZ"
      },
      "source": [
        "## Reading input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNCedpigCK-K",
        "outputId": "ad9d31c6-8a68-43ed-9081-39d77e77c004"
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'data/train/',\n",
        "    batch_size=batch_size,\n",
        "    class_names=classes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 210000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo3kdedFG7Ru",
        "outputId": "285e6af0-e405-40db-c7cb-532acafb5f79"
      },
      "source": [
        "# Look at some example sequences\n",
        "for seq_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"Sequence\", seq_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence b'N N G G C A T A A G A G T G T G T G T G T G T G T G T G T G T G T G T G T G T A T A C A C A C A A A C T G T A T A T A T G T G C T T G T G T G T A A T T G A A A T A C A T A T G T C A C A T A T C A T A C G T A C T A T G G C A T A T T A G G G T G T A A T A T T T A T A T T T G T G T A T A T A A T A A T A T T G A T G T T A A T G A A G T A T C C A T C A G T T T C T A T T T T C T T T A G T T T G T N N N'\n",
            "Label 0\n",
            "Sequence b'N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N T A T G G C T C A C T G C A G C C T G G A A C T C G G G C T C A A G C G A T C C T C C C A C C T C A G C C T C C T G A G T A G C T G G G A C T A C A G G T G T G T G C C A C C A A A T C C A G C T A A T T T T T G N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N'\n",
            "Label 0\n",
            "Sequence b'A T G T A G A T G A C A A A A T C T G A G A G T T A T T T A T T T A A T A A A G A G A G G T T T G T A T G T G T G T G G G G G T G T G G G T G T G G G G G A A G G G G G G T T G T G T G A G T G T G A T G T G T G A C G T G T T A A A C C G T G T A T T A A A G T C T C T C A A G T C T G T G A G G G G G T T T A A A T A G G G T A C G T G A G G T G T A A T T C T C A G A G A C T A G A C C T C A T A G A A A'\n",
            "Label 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGDXnUOpG6h_",
        "outputId": "eb21c44d-0e76-431e-971d-682b664e197b"
      },
      "source": [
        "# See labels\n",
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label 0 corresponds to notpresent\n",
            "Label 1 corresponds to present\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fopXneJoCQ8-",
        "outputId": "52c0315d-5274-4c2a-e958-9eb97c72d43e"
      },
      "source": [
        "raw_valid_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'data/valid/',\n",
        "    batch_size=batch_size,\n",
        "    class_names=classes)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 90000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjAlPn-MCgSv"
      },
      "source": [
        "vectorize_layer = TextVectorization(output_mode='int')\n",
        "\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)\n",
        "# vectorize_layer.set_vocabulary(vocab=np.asarray(['a', 'c', 't', 'g', 'n'])) -> why do I not want to use this?"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAl8xIY_CiFt"
      },
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label\n",
        "  # originally with one-hot encoding: return vectorize_layer(text)-2, label -> why it is different in embedding?\n",
        "\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "valid_ds = raw_valid_ds.map(vectorize_text)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7U4WV-MuITkk",
        "outputId": "ddbe98d1-c824-4cfd-ca59-e36f67a1094e"
      },
      "source": [
        "# retrieve a batch (of 32 sequences and labels) from the dataset\n",
        "seq_batch, label_batch = next(iter(raw_train_ds))\n",
        "first_seq, first_label = seq_batch[0], label_batch[0]\n",
        "print(\"Seq\", first_seq)\n",
        "print(\"Label\", raw_train_ds.class_names[first_label])\n",
        "print(\"Vectorized seq\", vectorize_text(first_seq, first_label))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seq tf.Tensor(b'N N N G G G C A G G G T A T C T G T G T C C G G T T G G G T C G G A A G C G A G G A A G G A C G G A C G G A C A G A G A G G G A C C G G G G A G A C G G G T G G G G A T G G G G C G A C G T C C T G T G G G G G T T C T A C T T T C C C G T T C C T C T T C C T C C C G G A C C C C C T G G A C C G A T G G T C C T G A A G G C C T G T T T G T C A A C A G T C C G T G A G C C G T A C G G G G C T C C A C N N', shape=(), dtype=string)\n",
            "Label present\n",
            "Vectorized seq (<tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
            "array([[6, 6, 6, 3, 3, 3, 5, 2, 3, 3, 3, 4, 2, 4, 5, 4, 3, 4, 3, 4, 5, 5,\n",
            "        3, 3, 4, 4, 3, 3, 3, 4, 5, 3, 3, 2, 2, 3, 5, 3, 2, 3, 3, 2, 2, 3,\n",
            "        3, 2, 5, 3, 3, 2, 5, 3, 3, 2, 5, 2, 3, 2, 3, 2, 3, 3, 3, 2, 5, 5,\n",
            "        3, 3, 3, 3, 2, 3, 2, 5, 3, 3, 3, 4, 3, 3, 3, 3, 2, 4, 3, 3, 3, 3,\n",
            "        5, 3, 2, 5, 3, 4, 5, 5, 4, 3, 4, 3, 3, 3, 3, 3, 4, 4, 5, 4, 2, 5,\n",
            "        4, 4, 4, 5, 5, 5, 3, 4, 4, 5, 5, 4, 5, 4, 4, 5, 5, 4, 5, 5, 5, 3,\n",
            "        3, 2, 5, 5, 5, 5, 5, 4, 3, 3, 2, 5, 5, 3, 2, 4, 3, 3, 4, 5, 5, 4,\n",
            "        3, 2, 2, 3, 3, 5, 5, 4, 3, 4, 4, 4, 3, 4, 5, 2, 2, 5, 2, 3, 4, 5,\n",
            "        5, 3, 4, 3, 2, 3, 5, 5, 3, 4, 2, 5, 3, 3, 3, 3, 5, 4, 5, 5, 2, 5,\n",
            "        6, 6]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvuci1GCI4cF",
        "outputId": "fe662f78-2898-4b7e-e6f7-d834f9cac31c"
      },
      "source": [
        "# lookup the token (string) that each integer corresponds to \n",
        "print(\"0 ---> \",vectorize_layer.get_vocabulary()[0]) # why there is no 0? Look below to see what I mean, all numbers have assigned nukleotide but not 0\n",
        "print(\" 1 ---> \",vectorize_layer.get_vocabulary()[1])\n",
        "print(\" 2 ---> \",vectorize_layer.get_vocabulary()[2])\n",
        "print(\" 3 ---> \",vectorize_layer.get_vocabulary()[3])\n",
        "print(\" 4 ---> \",vectorize_layer.get_vocabulary()[4])\n",
        "print(\" 5 ---> \",vectorize_layer.get_vocabulary()[5])\n",
        "print(\" 6 ---> \",vectorize_layer.get_vocabulary()[6])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 --->  \n",
            " 1 --->  [UNK]\n",
            " 2 --->  a\n",
            " 3 --->  g\n",
            " 4 --->  t\n",
            " 5 --->  c\n",
            " 6 --->  n\n",
            "Vocabulary size: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJlA5ndtCqm9"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyX1SfxPCr8Y"
      },
      "source": [
        "# one-hot encoding - NOT USED\n",
        "onehot_layer = keras.layers.Lambda(lambda x: tf.one_hot(tf.cast(x,'int64'), 4))\n",
        "\n",
        "model_lstm = tf.keras.Sequential([\n",
        "    onehot_layer,\n",
        "    keras.layers.LSTM(32, return_sequences=True),\n",
        "    keras.layers.LSTM(32, return_sequences=False),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")]) \n",
        "\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2clTJEjD3P0"
      },
      "source": [
        "# embedding\n",
        "embedding_dim = 16\n",
        "max_features = 7 # is 7 ok? I set it to the vocabulary size as in the text classification example mentioned at the beggining\n",
        "\n",
        "model_lstm = tf.keras.Sequential([\n",
        "    keras.layers.Embedding(max_features + 1, embedding_dim), # why +1 ?\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.LSTM(32, return_sequences=True),\n",
        "    keras.layers.LSTM(32, return_sequences=False),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")]) \n",
        "\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r6D0i-WC-he",
        "outputId": "a3518bfa-7eb3-4d4d-d393-9d4c08522fa0"
      },
      "source": [
        "epochs = 10 \n",
        "history = model_lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=epochs,\n",
        "    validation_data = valid_ds)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1641/1641 [==============================] - 123s 75ms/step - loss: 0.3205 - accuracy: 0.8784 - val_loss: 0.2532 - val_accuracy: 0.9162\n",
            "Epoch 2/10\n",
            "1641/1641 [==============================] - 127s 77ms/step - loss: 0.2295 - accuracy: 0.9209 - val_loss: 0.1743 - val_accuracy: 0.9366\n",
            "Epoch 3/10\n",
            "1641/1641 [==============================] - 122s 74ms/step - loss: 0.1561 - accuracy: 0.9425 - val_loss: 0.1352 - val_accuracy: 0.9485\n",
            "Epoch 4/10\n",
            "1641/1641 [==============================] - 126s 77ms/step - loss: 0.1346 - accuracy: 0.9486 - val_loss: 0.1202 - val_accuracy: 0.9548\n",
            "Epoch 5/10\n",
            "1641/1641 [==============================] - 128s 78ms/step - loss: 0.1205 - accuracy: 0.9546 - val_loss: 0.1183 - val_accuracy: 0.9566\n",
            "Epoch 6/10\n",
            "1641/1641 [==============================] - 128s 78ms/step - loss: 0.1112 - accuracy: 0.9584 - val_loss: 0.1043 - val_accuracy: 0.9621\n",
            "Epoch 7/10\n",
            "1641/1641 [==============================] - 128s 78ms/step - loss: 0.1009 - accuracy: 0.9626 - val_loss: 0.0951 - val_accuracy: 0.9654\n",
            "Epoch 8/10\n",
            "1641/1641 [==============================] - 127s 77ms/step - loss: 0.0942 - accuracy: 0.9648 - val_loss: 0.0890 - val_accuracy: 0.9671\n",
            "Epoch 9/10\n",
            "1641/1641 [==============================] - 128s 78ms/step - loss: 0.0888 - accuracy: 0.9670 - val_loss: 0.0948 - val_accuracy: 0.9645\n",
            "Epoch 10/10\n",
            "1641/1641 [==============================] - 129s 79ms/step - loss: 0.0852 - accuracy: 0.9682 - val_loss: 0.0850 - val_accuracy: 0.9684\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}